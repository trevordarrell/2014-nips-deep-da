Our adaptation experiments show that, despite its large size, even ImageNet is not large enough to cover all domains, and
that traditional domain adaptation methods go a long way in increasing performance and mitigating the effects of this shift.
% It is important not only to evaluate overall method performance, but to understand when each adaptation method can and should be applied.
Depending on the characteristics of the problem at hand, our results suggest different methods may be most suitable.

If no labels exist in the target domain, then there are unsupervised adaptation algorithms that are easy to use and fast to compute at adaptation time, yet still achieve increased performance over source-only methods. 
For this scenario, we experimented a subspace alignment based method that requires setting a parameter that indicates the dimensionality of the input subspaces. 
Figure~\ref{fig:sagfk-eval} shows the effect that changing the subspace dimensionality has on the overall method performance. 
In general, we noticed that these methods were not particularly sensitive to this parameter so long as the dimensionality remains larger than the number of categories in our label set.
Below this threshold, the subspace is less likely to capture all important discriminative information needed for classification.

In the case where we have a large source dataset and a limited number of labeled target examples, it may be preferable to compute source classifier parameters in advance, then examine only the source parameters and the target data at adaptation time.
This would correspond to using our late fusion approach which is unaffected by the number of data points in the source domain at adaptation time, and can thus be applied quickly.
However, this method requires setting a hyper parameter. 
Figure~\ref{fig:linint-eval} shows the performance of linear interpolation Late Fusion as we vary the hyperparameter $\alpha$ (combination parameter).
Although the method is sensitive to $\alpha$, we found that for both source domains, the basic strategy of setting $\alpha$ around $0.8$ provides a close approximation to optimal performance. 
This setting can be interpreted as trusting the target classifier more than the source, but not so much as to completely discount the information available from the source classifier. In each table we report the performance of linear interpolation averaged across hyper parameter settings $\alpha \in [0,1]$ for fairness.

If there are no computational constraints and there are very few labels in
the target domain, the best-performing method semi-supervised adaptation method is to use our early fusion technique with a classifier trained using the ``frustratingly easy'' approach originally proposed by \daume~\cite{daume} and applied again for
unsupervised deep models in \cite{ref:dlid}.

