% evaluation description
\label{sec:eval}

%\et{Yangqing says something should go here, but Kate thinks the paragraph we had
%here before was repetitive, so TODO: come up with a non-repetitive paragraph to
%introduce this section}

%KS this was repetitive
%We now present experiments to evaluate the effectiveness of deep learned
%representations for domain adaptation. Using a subset of the
%ImageNet~\cite{ilsvrc2012} and Office~\cite{saenko-eccv10} datasets, we evaluate
%multi-class accuracy in a one-shot supervised domain adaptation setting where
%one labeled example of each target category is provided at adaptation time. We
%also evaluate the effectiveness of unsupervised adaptation methods, which use
%only the unlabeled test data in the target domain to perform adaptation.

\subsection{Datasets}
The Office~\cite{saenko-eccv10} dataset is a collection of images from three
distinct domains: Amazon, DSLR, and Webcam. The 31 categories in the dataset
consist of objects commonly encountered in office settings, such as keyboards,
file cabinets, and laptops. Of these 31 categories, 16 overlap with the
categories present in the 1000-category ImageNet classification task\footnote{
The 16 overlapping categories are
\textit{backpack},
 \textit{bike helmet},
 \textit{bottle},
 \textit{desk lamp},
 \textit{desktop computer},
 \textit{file cabinet},
 \textit{keyboard},
 \textit{laptop computer},
 \textit{mobile phone},
 \textit{mouse},
 \textit{printer},
 \textit{projector},
 \textit{ring binder},
 \textit{ruler},
 \textit{speaker},
 and
 \textit{trash can}.
}.
Thus, for our experiments, we limit ourselves to these
16 classes.  In our experiments using Amazon as a source domain,
we follow the standard training protocol for this dataset of using 20 source
examples per category~\cite{saenko-eccv10,gong-cvpr12}, for a total of 320
images.

ImageNet~\cite{ilsvrc2012} is the largest available dataset of image category labels. We use 1000 categories' worth of data (1.2M images) to train the network, and use the 16 categories that overlap with Office (approximately 1200 examples per category or  $\approx$20K images total) as labeled source classifier data.

\subsection{Experimental Setup \& Baselines}

% \ks{I reorganized and moved some dataset details here from next sec}
For our experiments, we use the fully trained deep CNN model described in Section~\ref{sec:decaf}, extracting feature
representations from three different layers of the CNN. We then train a source classifier using these features on one of two source domains, and adapt to the target domain.

The source domains we consider are either the Amazon domain, or the corresponding 16-category ImageNet subset where each category has many more examples.
We focus on the Webcam domain as our target (test) domain, as Amazon-to-Webcam  was shown to be the only challenging shift in \cite{deeplearning-arxiv-2013} (the DSLR domain is much more similar to Webcam and did not require adaptation when using deep mid-level features). This combination exemplifies the shift from online web images to real-world images taken in typical office/home environments.
Note that, regardless of the source domain chosen to learn the classifier, ImageNet data from all 1000 categories was used to train the network. 

In addition, for the supervised adaptation setting we assume access to only a single example per category from the target domain (Webcam). 

%\ks{is this right? is this the standard test set?}
Each method is then evaluated across 20 random train/test splits, and we report averages and standard errors for each setting. 
For each random train/test split we choose one example for training and 10 other examples for testing (so there is a balanced test set across categories). Therefore, each test split has 160 examples. The unsupervised adaptation methods operate in a transductive setting, so the target subspaces are learned from the unlabeled test data.



\paragraph{Non-adaptive Baselines}
In addition to the adaptation methods outlined in Section~\ref{sec:adapt-algs},
we also evaluate using the following non-adaptive baselines.

\begin{itemize}
  \item{\textbf{SVM (source only)}: A support vector machine trained only on
    source data. 
%\ks{why not the CNN itself, in case of imagenet? why re-train and SVM?}
}
  \item{\textbf{SVM (target only)}: A support vector machine trained only on
    target data.}
  \item{\textbf{SVM (source and target)}: A support vector machine trained on
  both source and target data. To account for the large discrepancy between the
  number of training data points in the source and target domains, we weighted
  the data points such that the constraints from the source and target domains
  effectively contribute equally to the optimization problem.  Specifically,
  each source data point receives a weight of $\frac{n_t}{n_s+n_t}$, and each target
  data point receives a weight of $\frac{n_s}{n_s+n_t}$, where $n_s,n_t$ denote the
  number of data points in the source and target, respectively.}
\end{itemize}

Many of the adaptation methods we evaluate have hyperparameters that must be
cross-validated for use in practice, so we set the parameters of the adaptation
techniques as follows.

First, the C value used for C-SVM in the classifier for all methods is set to $C=1$. Without any validation data we are not able to tune this parameter properly, so we choose to leave it as the default value. Since all methods we report require setting of this parameter, we feel that the relative comparisons between methods is sound even if the absolute numbers could be improved with a new setting for C.
%\begin{itemize}
  %\item{
  For \daume and MMDT, which look at the source and target data
  simultaneously, we use the same weighting scheme as we did for the source and
  target SVM.
  %}
  %\item{
  Late Fusion with the linear interpolation combination rule is reported across hyperparameter settings in Figure~\ref{fig:linint-eval} to help understand how performance varies as we trade off emphasis between the learned classifiers from the source and target domains. Again, we do not have the validation data to tune this parameter so we report in the tables the performance averaged across parameter settings. The plot vs $\alpha$ indicates that there is usually a best parameter setting that could be learned with more available data.
  %}
  %\item{
  For PMT, we choose $\Gamma=1000$, which corresponds to allowing a large amount of transfer from the source classifier to the target classifier. We do this because the source-only classifier is
  stronger than the target-only classifier (with ImageNet source). 
  %}
  %\item{
  For the unsupervised methods GFK and SA, again we evaluated a variety of
  subspace dimensionalities and Figure~\ref{fig:sagfk-eval} shows that the overall method performance does not vary significantly with the dimensionality choice.
  %}
%\end{itemize}

\input{tables/amazon_fc6and7_unsup_table}
\input{tables/amazon_fc6and7_sup_table}

\subsection{Effect of Source Domain Size}
Previous studies considered source domains from the Office dataset. In this section, we ask what happens when an orders-of-magnitute larger source dataset is used.

For completeness we begin by evaluating Amazon as a source domain. 
Preliminary results on this setting are reported in~\cite{deeplearning-arxiv-2013}, here 
we extend the comparison here by
presenting the results with more adaptation algorithms and more complete
evaluation of hyperparameter settings. Table~\ref{tab:fc6and7_amazon} presents % TODO fix reference
multiclass accuracies for each algorithm using either layer 6 or 7 from the deep
network, which corresponds to the output from each of the fully connected layers.

An SVM trained using only Amazon data achieves 78.6\% in-domain accuracy (tested on the same domain) when using the DeCAF$_6$ feature and 80.2\% in-domain accuracy when using the DeCAF$_7$ feature. These numbers are significantly higher than the performance of the same classifier on Webcam test data, indicating that even with the DeCAF features, there is a still a domain shift between the Amazon and Webcam datasets. 
%This agrees with a similar result previously shown by Donahue et al.~\cite{deeplearning-arxiv-2013}.

Next, we consider an unsupervised adaptation setting where no labeled examples are available from the target dataset. In this scenario, we apply two state-of-the-art unsupervised adaptation methods, GFK~\cite{gong-cvpr12} and SA~\cite{sa}. 
Both of these methods make use of a subspace dimensionality hyperparameter.
We show the results using a 100-dimensional subspace and leave the discussion of setting this parameter until Section~\ref{sec:analysis}. For this shift the adaptation algorithms increase performance when using the layer 6 feature, but offer no additional improvement when using the layer 7 feature. 

We finally assume that a single example per category is available in the target domain.
As the bottom rows of Table~\ref{tab:fc6and7_amazon} show, supervised adaptation algorithms are able to provide significant improvement regardless of the feature space chosen, even in the one-shot scenario. % TODO: fix reference
For this experiment we noticed that using the second fully connected layer (DeCAF$_7$) was a stronger overall feature in general.

\subsection{Adapting with a Large Scale Source Domain}

We next address one of the main questions of this paper: Is there still a domain shift when using a large source dataset such as ImageNet? To begin to answer this question we follow the same experimental paradigm as the previous experiment, but use ImageNet as our source dataset. 
The results are shown in Table~\ref{tab:fc6and7_imagenet}. % TODO fix reference
\input{tables/imagenet_fc6and7_unsup_table}
\input{tables/imagenet_fc6and7_sup_table}

% \ks{this paragraph is easy to lose.. isn't this the main point of the paper?}
Again, we first verify that the source only SVM achieves higher performance when tested on in-domain data than on Webcam data. 
Indeed, for the 16 overlapping labels, the source SVM produces 62.50\% accuracy on ImageNet data using DeCAF$_6$ features and 74.50\% accuracy when using DeCAF$_7$ features. 
Compare this to the 54\% and 59\% for Webcam evaluation and a dataset bias is still clearly evident.

Note that when using ImageNet as a source domain, overall performance of all algorithms improves. In addition, unsupervised adaptation approaches are more effective than for the smaller source domain experiment. 

%In particular, for PMT we set $\Gamma=100$, \et{more parameters later}. Any methods that assign weights to the training data use the same balanced weighting scheme as before.


\subsection{Adapting a Pre-trained Classifier to a New Label Set }
\input{tables/fc8_unsup_table}
\input{tables/fc8_sup_table}

DeCAF$_8$ differs from the other DeCAF features in that it constitutes the 1000
activations corresponding to the 1000 labels in the ImageNet classification
task. In the CNN proposed by~\cite{supervision}, these activations are fed into
a softmax unit to compute the label probabilities. We instead experiment with
using the DeCAF$_8$ activations directly as a feature representation, which is
akin to training another classifier using the output of the 1000-way CNN
classifier.

Table~\ref{tab:fc8} shows results for various adaptation techniques using both % TODO fix reference
ImageNet and Amazon as source domains. We use the same setup as before, but
instead use DeCAF$_8$ as the feature representation. 
%KS took out -- this wasnt obvious to me!
%Unsurprisingly, 
The ImageNet results are uniformly better with DeCAF$_8$ than with DeCAF$_6$ or
DeCAF$_7$, likely due to the fact that DeCAF$_8$ was explicitly
trained on ImageNet data to effectively discriminate between ImageNet categories.
Because it can more
effectively classify images from the source domain, it is able to better adapt
from the source domain to the target domain.

However, we see a negligible difference in performance for Amazon, with
performance actually decreasing with respect to DeCAF$_7$ for certain adaptation
methods. We believe this is because the final activation vector is too specific
to the 1000-way ImageNet task, and that DeCAF$_7$ provides a more general
representation that is better suited to the Amazon domain. This, in turn,
results in improved adaptation.
In general, however, the difference between the
various DeCAF representations with Amazon as a source are small enough to be
insignificant.

\subsection{Analysis and Practical Considerations}
\label{sec:analysis}
\input analysis


\input linint-fig
