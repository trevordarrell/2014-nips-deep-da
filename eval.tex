% evaluation description
\label{sec:eval}

%\et{Yangqing says something should go here, but Kate thinks the paragraph we had
%here before was repetitive, so TODO: come up with a non-repetitive paragraph to
%introduce this section}

%KS this was repetitive
%We now present experiments to evaluate the effectiveness of deep learned
%representations for domain adaptation. Using a subset of the
%ImageNet~\cite{ilsvrc2012} and Office~\cite{saenko-eccv10} datasets, we evaluate
%multi-class accuracy in a one-shot supervised domain adaptation setting where
%one labeled example of each target category is provided at adaptation time. We
%also evaluate the effectiveness of unsupervised adaptation methods, which use
%only the unlabeled test data in the target domain to perform adaptation.

\subsection{Datasets}
The Office~\cite{saenko-eccv10} dataset is a collection of images from three
distinct domains: Amazon, DSLR, and Webcam. The 31 categories in the dataset
consist of objects commonly encountered in office settings, such as keyboards,
file cabinets, and laptops. Of these 31 categories, 16 overlap with the
categories present in the 1000-category ImageNet classification task\footnote{
The 16 overlapping categories are
\textit{backpack},
 \textit{bike helmet},
 \textit{bottle},
 \textit{desk lamp},
 \textit{desktop computer},
 \textit{file cabinet},
 \textit{keyboard},
 \textit{laptop computer},
 \textit{mobile phone},
 \textit{mouse},
 \textit{printer},
 \textit{projector},
 \textit{ring binder},
 \textit{ruler},
 \textit{speaker},
 and
 \textit{trash can}.
}.
Thus, for our experiments, we limit ourselves to these
16 classes.  In our experiments using Amazon as a source domain,
we follow the standard training protocol for this dataset of using 20 source
examples per category~\cite{saenko-eccv10,gong-cvpr12}, for a total of 320
images.

ImageNet~\cite{ilsvrc2012} is the largest available dataset of image category labels. We use 1000 categories' worth of data (1.2M images) to train the network, and use the 16 categories that overlap with Office (approximately 1200 examples per category or  $\approx$20K images total) as labeled source classifier data.

\subsection{Experimental Setup \& Baselines}

% \ks{I reorganized and moved some dataset details here from next sec}
For our experiments, we use the fully trained deep CNN model described in Section~\ref{sec:decaf}, extracting feature
representations from three different layers of the CNN. We then train a source classifier using these features on one of two source domains, and adapt to the target domain.

The source domains we consider are either the Amazon domain, or the corresponding 16-category ImageNet subset where each category has many more examples.
We focus on the Webcam domain as our target (test) domain, as Amazon-to-Webcam  was shown to be the only challenging shift in \cite{deeplearning-arxiv-2013} (the DSLR domain is much more similar to Webcam and did not require adaptation when using deep mid-level features). This combination exemplifies the shift from online web images to real-world images taken in typical office/home environments.
Note that, regardless of the source domain chosen to learn the classifier, ImageNet data from all 1000 categories was used to train the network. 

In addition, for the supervised adaptation setting we assume access to only a single example per category from the target domain (Webcam). 

%\ks{is this right? is this the standard test set?}
Each method is then evaluated across 20 random train/test splits, and we report averages and standard errors for each setting. 
For each random train/test split we choose one example for training and 10 other examples for testing (so there is a balanced test set across categories). Therefore, each test split has 160 examples. The unsupervised adaptation methods operate in a transductive setting, so the target subspaces are learned from the unlabeled test data.



\paragraph{Non-adaptive Baselines}
In addition to the adaptation methods outlined in Section~\ref{sec:adapt-algs},
we also evaluate using the following non-adaptive baselines.

\begin{itemize}
  \item{\textbf{SVM (source only)}: A support vector machine trained only on
    source data. 
%\ks{why not the CNN itself, in case of imagenet? why re-train and SVM?}
}
  \item{\textbf{SVM (target only)}: A support vector machine trained only on
    target data.}
  \item{\textbf{SVM (source and target)}: A support vector machine trained on
  both source and target data. To account for the large discrepancy between the
  number of training data points in the source and target domains, we weighted
  the data points such that the constraints from the source and target domains
  effectively contribute equally to the optimization problem.  Specifically,
  each source data point receives a weight of $\frac{n_t}{n_s+n_t}$, and each target
  data point receives a weight of $\frac{n_s}{n_s+n_t}$, where $n_s,n_t$ denote the
  number of data points in the source and target, respectively.}
\end{itemize}

Many of the adaptation methods we evaluate have hyperparameters that must be
cross-validated for use in practice, so we set the parameters of the adaptation
techniques as follows.

First, the C value used for C-SVM in the classifier for all methods is set to $C=1$. Without any validation data we are not able to tune this parameter properly, so we choose to leave it as the default value. Since all methods we report require setting of this parameter, we feel that the relative comparisons between methods is sound even if the absolute numbers could be improved with a new setting for C.
%\begin{itemize}
  %\item{
  For \daume and MMDT, which look at the source and target data
  simultaneously, we use the same weighting scheme as we did for the source and
  target SVM.
  %}
  %\item{
  Late Fusion with the linear interpolation combination rule is reported across hyperparameter settings in Figure~\ref{fig:linint-eval} to help understand how performance varies as we trade off emphasis between the learned classifiers from the source and target domains. Again, we do not have the validation data to tune this parameter so we report in the tables the performance averaged across parameter settings. The plot vs $\alpha$ indicates that there is usually a best parameter setting that could be learned with more available data.
  %}
  %\item{
  For PMT, we choose $\Gamma=1000$, which corresponds to allowing a large amount of transfer from the source classifier to the target classifier. We do this because the source-only classifier is
  stronger than the target-only classifier (with ImageNet source). 
  %}
  %\item{
  For the unsupervised methods GFK and SA, again we evaluated a variety of
  subspace dimensionalities and Figure~\ref{fig:sagfk-eval} shows that the overall method performance does not vary significantly with the dimensionality choice.
  %}
%\end{itemize}

\subsection{Effect of a High Complexity Model}

\input{tables/amazon_fc8_unsup_table}
\input{tables/amazon_fc8_sup_table}

We begin by evaluating the effect that a high complexity model (in this case,
a fully trained convolutional net) has on mitigating domain shift.

We evaluate using Amazon as a source domain. 
Preliminary results on this setting are reported in~\cite{deeplearning-arxiv-2013}, but here 
we extend the comparison here by
presenting the results with more adaptation algorithms and more complete
evaluation of hyperparameter settings. Tables~\ref{tab:amazon_fc8_unsup} and \ref{tab:amazon_fc8_sup} present % TODO fix reference
multiclass accuracies for each algorithm using layer 8 from the deep
network, which corresponds to the output from the final fully connected layer.

\et{this paragraph needs to be redone with fc8.} An SVM trained using only Amazon data achieves 78.6\% in-domain accuracy (tested on the same domain) when using the DeCAF$_6$ feature and 80.2\% in-domain accuracy when using the DeCAF$_7$ feature. These numbers are significantly higher than the performance of the same classifier on Webcam test data, indicating that even with the DeCAF features, there is a still a domain shift between the Amazon and Webcam datasets. 
%This agrees with a similar result previously shown by Donahue et al.~\cite{deeplearning-arxiv-2013}.

Next, we consider an unsupervised adaptation setting where no labeled examples are available from the target dataset. In this scenario, we apply two state-of-the-art unsupervised adaptation methods, GFK~\cite{gong-cvpr12} and SA~\cite{sa}. 
Both of these methods make use of a subspace dimensionality hyperparameter.
We show the results using a 100-dimensional subspace in Table~\ref{tab:amazon_fc8_unsup} and leave the discussion of setting this parameter until Section~\ref{sec:analysis}.

We finally assume that a single example per category is available in the target domain.
As Table~\ref{tab:amazon_fc8_sup} shows, supervised adaptation algorithms are able to provide significant improvement, even in the one-shot scenario. % TODO: fix reference

\input{tables/full_office_table}

For the purposes of comparison to prior work, we also ran a set of experiments
using the standard evaluation protocol for the office dataset. \et{Gotta explain what
the typical protocol is.} Results from these experiments can be seen in
Table~\ref{tab:full_office}.

\et{There will be an analysis of the results here once these experiments have
been run.}

\subsection{Adapting with a Large Scale Source Domain}

We next address one of the main questions of this paper: Is there still a domain shift when using a large source dataset such as ImageNet? To begin to answer this question we follow the same experimental paradigm as the previous experiment, but use ImageNet as our source dataset. 
The results are shown in Tables~\ref{tab:imagenet_fc8_unsup} and \ref{tab:imagenet_fc8_sup}. % TODO fix reference
\input{tables/imagenet_fc8_unsup_table}
\input{tables/imagenet_fc8_sup_table}

% \ks{this paragraph is easy to lose.. isn't this the main point of the paper?}
\et{This paragraph also needs to be redone with FC8.}
Again, we first verify that the source only SVM achieves higher performance when tested on in-domain data than on Webcam data. 
Indeed, for the 16 overlapping labels, the source SVM produces 62.50\% accuracy on ImageNet data using DeCAF$_6$ features and 74.50\% accuracy when using DeCAF$_7$ features. 
Compare this to the 54\% and 59\% for Webcam evaluation and a dataset bias is still clearly evident.

Note that when using ImageNet as a source domain, overall performance of all algorithms improves. In addition, unsupervised adaptation approaches are more effective than for the smaller source domain experiment. 

\subsection{Analysis and Practical Considerations}
\label{sec:analysis}
\input analysis


\input linint-fig
